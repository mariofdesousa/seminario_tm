---
title: "Seminário - Mineração de Texto"
author: "Fabiana, Fernando, Mário, Rafael e Rodney"
date: "Junho de 2017"
output:
  html_document:
    css: button.css
    highlight: haddock
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
bibliography: bibliografia.bib
---
<script src="hideOutput.js"></script>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Metodologia


Sabemos que com a popularização da internet, um enorme volume dados é gerado a cada minuto. Durante anos, aprendemos a tabular e quantificar variáveis acomodadas em bancos de dados, porém os tempos são outros.

Estamos trabalhando com o estudo computacional de opiniões, sentimentos e emoções expressos em texto, e para tal, será preciso criar métricas para contabilização dos mesmos.

## Contagem e frequência de palavras

Segundo GOMES (2013), o desafio é saber como manipular essa grande quantidade de informação gerada e investigar como as organizações podem se beneficiar dessas informações, considerando que 80% das informações das organizações estão contidas em documentos de texto (TAN, 1999). 

Uma vez contabilizada a quantidade de palavras que aparecem dentro de um banco de dados pré estabelecido, é possível calcular a freqüência, percentual, das palavras e utilizar o pacote $\texttt{ggplot2}$ para elaboração de um histograma para visualização. 
Outra opção de identificação de concentração de palavras na análise de sentimentos, são as nuvens de palavras, que são imagens que demonstram de maneira visual, a frequência da ocorrência das palavras em um dado texto: quanto maior for o número de ocorrências de uma palavra, maior a mesma será na nuvem de palavras. Depois de criarmos um arquico com a contagem das palavras, devemos intalar um arquivo Word.cloud

### Núvem de palavras

Um WordCloud é feito com as palavras mais utilizadas em um determinado texto ou documento, de forma com que fique visual e chamativo para as palavras mais importantes.

Para a nossa base de dados foram utilizados tweets da plataforma social "twitter", sendo que foram baixados 30.000(trinta mil) tweets, tendo a partir deles palavras relacionadas a depressão, sendo todas no idioma inglês, como "depression" e "anxiety".


Dado isso foi feita uma separação dos dados na linguagem R e os dados foram transformados em "Data Frame" para manipulação. Depois fazemos todas as limpezas necessárias para que o texto fique uniforme, como deixar todas as letras minísculas, remover os nomes de usuário, tirar pontuação, links e espaços em branco desnecessários, e por fim, números e palavras desnecessárias, como a identificação do aparelho pelo qual foi enviado o tweet.

## Descrição dos dados
A base de dados utilizada contém 30000 tweets extraídos com auxílio da função $\texttt{searchTwitter}$ do pacote $\texttt{twitteR}$. O banco de dados é composto por 10000 tweets escritos em inglês durante o período que compreende 01/01/2015 a 06/06/2017, contendo as palavras "depression", "depressive" e "depressed". A função $\texttt{searchTwitter}$ retorna uma lista com diversas informações, então faz-se necessário aplicar a função $\texttt{twListToPdf}$, também disponível no pacote $\texttt{twitteR}$, para transformar a lista do twitter em um dataframe compatível com os métodos  $\texttt{tidy}$ e $\texttt{tm}$.

## Análise de tópicos

Um interesse usual em análises de grandes volumes de textos é classicar os documentos em diferentes tópicos, permitindo idetenficar ou validar grupos entre os documentos analisados.  Uma forma de realizar esse tipo de análise é utilizar o método de alocação latente de Dirichlet (LDA, na sigla em inglês), que foi proposto por @blei2003latent. Esse método é bastante conhecido na literatura de mineração de texto pela sua eficiência, já tendo sido aplicado em análises de e-mails, livros digitais e notícias, por exemplo, como destacam @berry2010text.

As principais características do LDA, como apresentam @Silge2017Text, são as seguintes:

* Documentos diferentes são formados por misturas de tópicos: Dados dois documentos $A$ e $B$, e tópicos distintos 1 e 2, podemos ter que o $A$ é formado em 75% por conteúdo sobre o tópico 1 e em 25% sobre o tópico 2, enquanto $B$ é formado em 40% sobre o tópico 1 e 60% sobre o tópico 2, por exemplo.

* Diferentes palavras podem fazer parte de diferentes tópicos: a palavra "estudante" pode estar mais associada a um tópico sobre universidades enquanto que "cliente" pode ser mais associado a um tópico sobre empresas, mas o termo "dados" pode estar relacionado de maneira similar a ambos esses tópicos.

Para exemplificar a suposição do modelo LDA vamos considerar as seguintes notações: uma palavra $w$ é uma unidade discreta de um conjunto que forma o vocabulário utilizado; um documento $\mathbf{w} = \left(w_1,\ldots,w_N\right)$ é um conjunto de $N$ palavras; um corpo $D=\left\{\mathbf{w}_1,\ldots,\mathbf{w}_M\right\}$ é um conjunto de $M$ documentos. Adicionalmente, no LDA o número de tópicos existentes é assumido conhecido, mas os tópicos em si são variáveis latentes que não conhecemos e desejamos estimar. Dessa maneira, @blei2003latent consideram as seguintes suposições na geração do corpo:

1. Gera-se $N\sim \textrm{Pois}\left(\lambda\right)$;
2. Gera-se $\theta = \textrm{Dirichlet}\left(\mathbf{\alpha}\right)$, o qual será o vetor de probabilidades dos $k$ tópicos;
3. Para cada uma das $N$ palavras $w_n$:

+ Sorteia-se um tópico $z_n \sim \textrm{Multinomial}\left(k,\theta\right)$

+ Sorteia-se uma palavra $w_n$ de $p\left(w_n|z_n,\beta\right)$, que segue uma distribuição multinomial cujas probabilidades são condicionadas no tópico $z_n$ e em um vetor desconhecido de parâmetros $\beta$.


A aplicação do LDA apresentada por @blei2003latent é baseada em um modelo Bayesiano com três níveis hierárquicos, e permite estimar a probabilidade que cada palavra tem de pertencer à cada um dos tópicos, bem como estimar a probabilidade de cada documento ser sobre cada tópico.

No programa $\texttt{R}$ a análise de tópicos pode ser realizada através do pacote $\texttt{topicmodels}$, utilizando a função $\texttt{LDA}$. Uma aplicação dessa função do $\texttt{R}$ é fornecida por @Silge2017Text em um exemplo onde livros distintos são tratados como tópicos e seus respectivos capítulos como documentos; no exemplo dos autores, mostra-se que o LDA consegue classificar quase todos capítulos corretamente em seus respectivos livros, demonstrando a eficiência do método.

## Análise de sentimentos

Em análise de sentimentos o principal objetivo consiste em fazer "inferência"" sobre sentimetos expressos em forma de texto. @messias2017 cita algumas das recentes aplicações da referida metodologia, sendo as principais:

* Monitorar a reputação de alguma empresa ou marca;
* Fornecer perspectivas analíticas para investidores sobre determinado mercado (se certo mercado tem potencial ou não);
* Rastrear opiniões associadas a certos políticos (tais informações são úteis para o marketing do candidato) e
* Proporcionar meios de mensurar o bem-estar dos usuários de certo modelo de smartphone.

@goncalves2013 discute que não há a "melhor" técnica para se aplicar em análise de sentimentos. Conclusão semelhante foi obtida por @ribeiro2016 após comparar 22 métodos diferentes em 18 documentos onde não foi possível encontrar um método que fosse igualmente bom, em termos de classificação, para todos os textos. 

Na aplicação sobre depressão iremos considerar os dicionários de sentimentos **afinn**(@afinn2011), **bing**(@bing2005) e **nrc**(@nrc2013), por estarem disponíveis no R. Os três dicionários são compostos por unigramas, i.e., palavras únicas. Os vocabulários contém palavras em que são associados scores relativos a sentimentos positivos ou negativos. No caso do **nrc** há associação de palavras a sentimentos como alegria, raiva, tristeza, negatividade, surpresa, medo e desgosto. Esta classificação é feita de modo binário (sim ou não), se determinada palavra corresponde a determinado sentimento. Já no vocabulário **bing** as palavras são classificadas, também de modo binário, porém não há distinção de sentimentos. As palavras ou são associadas a sentimentos positivos ou a negativos. Por fim, o vocabulário **afinn** atribui scores que vão de -5 a 5 para as palavras. De modo que quanto mais próximo de -5 mais negativa é aquela palavra e, de modo análogo, quanto mais próximo de 5 mais posiva é a palavra. Os três vocabulários estão disponívels no $\texttt{R}$ no pacote $\texttt{tidytext}$. 

Geralmente os dicionários são elaborados pelos próprios autores com auxílio de crowdsourcing. É haver algum tipo de validação cruzada para testar o dicionário criado. Neste contexto fica evidente que os vocabulário são contemporâneos, ou seja, não é recomendado aplicar um dicionário de sentimentos atual para analisar um texto de séculos atrás. A análise pode ser feita, mas devem ser tomados os devidos cuidados ao analisar os achados.

Iniciamos a análise de sentimentos com o vocabulário **nrc** para obtermos palavras relacionadas a cada um dos sentimos, sendo eles alegria, raiva, tristeza, negatividade, surpresa, medo e desgosto, de modo que obtemos gráficos de contagem de palavras para cada um deles. Essa técnica permite avaliar quais palavras estão associadas a tais sentimentos no contexto de depressão. Prosseguimos a análise de sentimentos com o dicionário **bing**, sendo possível construir índices baseados nos tipos de palavras (positiva ou negativa) e na quantidade de ocorrência das mesmas em um grupo de $n$ linhas. Esse tipo de técnica permite avaliar alterações, caso existam, de sentimentos ao longo do texto. Note que a quantidade de linhas que serãoagrupadas depende do tipo e do tamanho do texto que está sendo análisado. Ainda utilizando o dicionário **bing**, apresentamos uma núvem de palavras de sentimentos, isto é, uma núvem de palavras com distinção entre palavras positivas e negativas. Prosseguimos a análise de sentimentos comparando os três dicionários, de modo a observar se há alguma discrepância em algum deles. 

## Dendrogramas

De acordo com o dicionário Merriam-Webster[^1], um dendograma se define como um diagrama em árvore que representa uma hierarquia de grupos de objetos, baseada em sua similiaridade ou características em comum.

No caso da análise textual, as palavras são os elementos que compõe os grupos do diagrama. A distância medida entre elas, por sua vez, depende da frequência de ocorrência de cada palavra. Definidos os grupos e sua respectiva hierarquia, o objetivo do dendrograma é facilitar a associação entre as palavras a fim de se extrair informações relevantes sobre o texto @t17.

Diversos métodos podem ser utilizados para o agrupamento de objetos. O dendrograma é o resultado em diagrama de um método aglomerativo hierárquico. Neste método, cada objeto da análise é inicialmente considerado como um grupo individual. Em seguida, objetos com pouco distância entre si são aglomerados em grupos maiores. Estes  grupos maiores, por seu turno, são reagrupados, novamente segundo a distância entre seus objetos. Ao final do processo, todos os grupos se aglomeram em apenas um. A vizualização deste processo é o próprio dendrograma \cite{http://www.stewartschultz.com/statistics/books/Cambridge%20Dictionary%20Statistics%204th.pdf}.


No R, existem seis métodos de mensuração da distância entre os objetos, no caso, a frequência das palavras e oito métodos de aglomeração. Dentre os métodos de cálculo da distância, podemos relevar


1) **Euclidiana**: o método padrão de distância entre vetores 
$$\sum{(x_i- y_i^2)}$$

2) **Máximos**: mede a distância máxima entre componentes de dois vetores:$$\max{x_i,y_i}$$

3) **Manhattan**: mede a distância absoluta entre dois vetores $$[{x_i, y_i}]$$

4) **Canberra**: definido pela seguinte expressão, onde termos nulos do denominador são excluídos da amostra: $$\sum\frac{(|x_i - y_i|}{ |x_i + y_i|}).$$ 

5) **Binário**: A distância é a proporção de vetores considerados binários, (*bits*), em que apenas um dos vetores está presente dentre aqueles em que apenas os vetores 1 estão incluídos.

6) **Minkowski**: a p-ésima raíz da soma dos das diferenças dos componentes elevada à p-ésima potência. $$\sqrt[p_{th}]{(x_i-y_i)^p_{th} }$$


Com relação à aglomeração, em cada estágio do processo a distância entre os grupos é recalculada pelo algorítimo de Lance-Willians \cite(https://pdfs.semanticscholar.org/7c16/8b8262acf2ec40a971604f273462136f4835.pdf), em acordo com o método de agrupamento selecionado. Os métodos implementados no R, segundo o pacote $\texttt{dentextend}$, são:"Ward","Single","Median","Centroid" e "Complete". 

Á título de ilustração, na próxima sessão se demonstrará a diferença entre os métodos, assim como sua aplicação para a análise de dados abertos do aplicativo Twitter.


### Construindo o Dendrograma

O primeiro passo para a construção do dendrograma para análise saúde mental a partir do Twitter, foi a "limpeza" do conjunto textual selecionado. Este procedimento tem como objetivo retirar palavras que não acrescentam significado próprio ao texto. Este conjunto de palavras, que recebe o nome de *stopwords*, ao mesmo tempo que não acrescentam valor semantico são muito frequentes por serem compostas por artigos, preposições e pronomes. No caso em questão, utilizou-se o método clássico, onde se utilzia uma lista pré-concebida de palavras para cada idioma @v15



Após a eliminação destas palavras, contruiu-se uma matriz de frequência das palavras e analisou-se as 50 palavras mais frequentes, número estimado como parâmetro máximo para a representação de um dendrograma. Notou-se que algumas palavras extras poderiam ser removidas como abreviações e gírias que as "stopwords" predefinidas não contemplavam\footnote[^2].

O passo seguinte foi a análise gráfica dos termos mais frequentes, a fim de se avaliar a frequências das palavras de forma relativa. O  abaixo representa as 50 palavras mais frequentes da base de dados, realizadas a limpeza previamente definida.

# Resultados e discussões

## Contagem de palavras

<div class="fold s o">
```{r, message=FALSE,results='hide',echo=TRUE}

if("dplyr" %in% rownames(installed.packages())==FALSE)
{install.packages("dplyr")};library(dplyr)

if("stringr" %in% rownames(installed.packages())==FALSE)
{install.packages("stringr")};library(stringr)

if("tidytext" %in% rownames(installed.packages())==FALSE)
{install.packages("tidytext")};library(tidytext)

if("tidyr" %in% rownames(installed.packages())==FALSE)
{install.packages("tidyr")};library(tidyr)

if("ggplot2" %in% rownames(installed.packages())==FALSE)
{install.packages("ggplot2")};library(ggplot2)

if("twitteR" %in% rownames(installed.packages())==FALSE)
{install.packages("twitteR")};library(twitteR)

if("tibble" %in% rownames(installed.packages())==FALSE)
{install.packages("tibble")};library(tibble)

if("wordcloud" %in% rownames(installed.packages())==FALSE)
{install.packages("wordcloud")};library(wordcloud)

udemytweets <-readRDS("depression_tweets_final.gzip")
tweets.df <- twListToDF(udemytweets) ##CALMA QUE VAI DEMORAR
tweets_tidy_df <- as_tibble(tweets.df)

# Limpando a base de dados

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg  <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

### Palavras retiradas para contagem
mystopwords_count <- c(stop_words$word,"depression","depressive","depressed",
                  "im","wcw","ur","depressed.","depression.","depressed,",
                  "depression,","me,","lol","#depression","me.","wow",
                  "depressed?","idk","ppl","bc","dont","lot","it.","anxiety,",
                  "her,","af","y'all","rn")
```
</div>

<div class="fold s">
```{r, message=FALSE,results='hide',echo=TRUE}
### Limpando a base para a contagem de palavras
tweets_tidy_count <- tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex") %>%
  filter(!word %in% mystopwords_count,
         str_detect(word, "[a-z]")) %>%
  anti_join(stop_words)%>%
  count(word,sort=TRUE)%>%
  filter(n > 100 & n < 500) %>%
  mutate(word = reorder(word, n)) 

  ggplot(tweets_tidy_count,aes(word, n)) +
  geom_col() +
  xlab("Contagem") +
  coord_flip()
```
</div>

**Interpretar**

<div class="fold s">
```{r, message=FALSE,results='hide',echo=TRUE}
tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex") %>%
  filter(!word %in% mystopwords_count,
         str_detect(word, "[a-z]")) %>%
  anti_join(stop_words)%>%
  count(word,sort=TRUE)%>%
  mutate(word = reorder(word, n)) %>%
  with(wordcloud(word, n, max.words = 100))
```
</div>

**Interpretar**

<div class="fold s">
```{r, message=FALSE,results='hide',echo=TRUE,warning=FALSE}
### Histograma da proporcao de palavras
### Palavras retiradas do histograma

mystopwords_hist <- c(stop_words$word,"im","wcw","ur",
                         "depressed.","depression.","depressed,")
tweets_tidy_df %>% 
    filter(!str_detect(text, "^RT"),
           text != "im",
           text != "ur",
           text != "af",
           text != "wcw",
           text != "shes",
           text != "booktweeter0",
           text != "bktwr",
           !str_detect(text, "[0-9]"),
           !str_detect(text, ":"),
           !str_detect(text, "textless")) %>%
    mutate(text = str_replace_all(text, replace_reg, "")) %>%
    unnest_tokens(word, text, token = "regex") %>%
    filter(!word %in% mystopwords_hist,
           str_detect(word, "[a-z]")) %>%
    anti_join(stop_words)%>%
    count(word,sort=TRUE)%>%
    mutate(word = reorder(word, n)) %>%
ggplot(aes(n/sum(n))) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  xlab("Proporção")+
  ylab("Contagem")
```
</div>

**interpretar**

### Núvem de palavras

<div class="fold s o">
```{r, message=FALSE,results='hide',echo=TRUE}
if("tm" %in% rownames(installed.packages())==FALSE)
{install.packages("tm")};library(tm)

#convert all text to lower case
tweets.text <- tolower(tweets.df)
# Replace blank space ("rt")
tweets.text <- gsub("rt", "", tweets.text)
# Replace @UserName
tweets.text <- gsub("@\\w+", "", tweets.text)
# Remove punctuation
tweets.text <- gsub("[[:punct:]]", "", tweets.text)
# Remove links
tweets.text <- gsub("http\\w+", "", tweets.text)
# Remove tabs
tweets.text <- gsub("[ |\t]{2,}", "", tweets.text)
# Remove blank spaces at the beginning
tweets.text <- gsub("^ ", "", tweets.text)
# Remove blank spaces at the end
tweets.text <- gsub(" $", "", tweets.text)
tweets.text <- gsub("href", "", tweets.text)
tweets.text <- gsub("relnofollowtwitter", "", tweets.text)
tweets.text <- gsub("iphonea", "", tweets.text)
tweets.text <- gsub("androida", "", tweets.text)
tweets.text <- gsub("false", "", tweets.text)
tweets.text <- gsub("wcw", "", tweets.text)
tweets.text <- gsub("ball", "", tweets.text)
tweets.text <- gsub("around", "", tweets.text)
tweets.text <- gsub("still", "", tweets.text)
tweets.text <- gsub("goes", "", tweets.text)

bpcorpus <- Corpus(VectorSource(tweets.text))
bpcorpus <- tm_map(bpcorpus, function(x) removeWords(x, stopwords()))
bpcorpus <- tm_map(bpcorpus, removeNumbers)
```
</div>

Existe um problema ao rodar o WordCloud aqui:a palavra "depression" aparece como "depressed", "depressive", ent?o vamos substituir.

<div class="fold s">
```{r, message=FALSE,results='hide',echo=TRUE}
tweets.text <- gsub("depressed", "depress", tweets.text)
tweets.text <- gsub("depressive", "depress", tweets.text)
tweets.text <- gsub("depression", "depress", tweets.text)
tweets.text <- gsub("depressednjonah", "depress", tweets.text)

wordcloud(bpcorpus,min.freq = 300, scale=c(8,0.1),colors=brewer.pal(8, "Dark2"),  
          random.color= TRUE, random.order = FALSE, max.words = 70)
```
</div>

Mas é importante mostrar como ele funciona, o pacote $\texttt{wordcloud}$ pega as palavras com as maiores frequências nos documentos e as coloca na maneira que miniminiza os espaços em branco de acordo com os tamanhos das palavras, que é proporcional à frequência que elas aparecem. Além disso, o usuário pode escolher manualmente as palavras e as frequências.

## Análise de sentimentos

<div class="fold s">
```{r, message=FALSE,results='hide',echo=TRUE}

mystopwords <- c(stop_words$word)

tidy_tweets <- tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex") %>%
  filter(!word %in% mystopwords,
         str_detect(word, "[a-z]")) %>%
  anti_join(stop_words)

nrneg <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

tidy_tweets %>%
  inner_join(nrneg) %>%
  count(word, sort = TRUE)%>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Negative") +
  ylab("Número de ocorrências")+
  coord_flip()
```
```{r message=FALSE,results='hide',echo=TRUE}
#####sadness    
nrsad <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

tidy_tweets %>%
  inner_join(nrsad) %>%
  count(word, sort = TRUE) %>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Sadness") +
  ylab("Número de ocorrências")+
  coord_flip()
```
```{r message=FALSE,results='hide',echo=TRUE}
#####         anger                      ####   
nrang <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")

tidy_tweets %>%
  inner_join(nrang) %>%
  count(word, sort = TRUE) %>%
  filter(n > 15 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Anger") +
  ylab("Número de ocorrências")+
  coord_flip() 
```
```{r message=FALSE,results='hide',echo=TRUE}
#####  fear      
nrfear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_tweets %>%
  inner_join(nrfear) %>%
  count(word, sort = TRUE) %>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Fear") +
  ylab("Número de ocorrências")+
  coord_flip() 
```
```{r message=FALSE,results='hide',echo=TRUE}
##### disgust 
nrdisg <- get_sentiments("nrc") %>% 
  filter(sentiment == "disgust")

tidy_tweets %>%
  inner_join(nrdisg) %>%
  count(word, sort = TRUE) %>%
  filter(n > 15 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Disgust") +
  ylab("Número de ocorrências")+
  coord_flip() 
```
</div>

Utilizando o dicionário **nrc**, observamos a frequência de palavras relacionadas aos sentimentos contidos no dicionário. O objetivo deste tipo de análise é verificar quais palavras estão associadas a quais sentimentos no âmbito da análise em questão.

É interessante destacar que palavras relacionadas a ansiedade, suicídio, morte e cansaço são recorrentes com alta frequência quando contrastamos os sentimentos "negative", "anger", "fear" e "disgust". Este resultado é interessante pois a literatura comumente não encontra alta frequência de palavras associadas a sentimentos distintos. A palavra "anxiety" merece atenção especial uma vez que há referências robustas relacionando ansiedade com depressão e, possivelmente, suicídio. 

<div class="fold s">
```{r}
#####surprise  
nrsurp <- get_sentiments("nrc") %>% 
  filter(sentiment == "surprise")

tidy_tweets %>%
  inner_join(nrsurp) %>%
  count(word, sort = TRUE) %>%
  filter(n > 15 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab("Surprise") +
  ylab("Número de ocorrências")+
  coord_flip() 
```
</div>

Ao comparar a frequência de palavras relacionadas ao sentimento "surprise" observa-se a presença de diversas palavras positivas. Um resultado curioso é a presença da palavra "Trump", uma óbvia referência ao presidente dos Estados Unidos Donald Trump. Ainda avaliado as ocorrências em "surprise" nota-se que "hope" é uma palavra que aparece com relativa frequência nos tweets relacionados com depressão. Obviamente, "money", também aparece com certa frequência

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
################# Dicionário Bing      
 tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, ""),
         line = row_number()) %>%
  unnest_tokens(word, text, token = "regex") %>%
  anti_join(stop_words) %>%
  filter(!word %in% mystopwords,
         str_detect(word, "[a-z]"))%>%
  inner_join(get_sentiments("bing"))%>%
  count(index = line %/% 150, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment)) +
  geom_col(show.legend = FALSE) +
  xlab("Index")+
  ylab("Score")
```
</div>

O gráfico acima, criado a partir do dicionário **bing** é conciso em informar que a base de dados em questão possui elevada aparição de palavras negativas. Note que no dicionário **nrc** encontramos certas palavras positivas associadas ao sentimento "surprise", contudo essas palavras não alteram a característica extremamente negativa do texto. Esta conclusão é obtida ao observarmos que em grupos de 150 não há nenhum com score positivo. 

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
################# Dicionário afinn              #################     

afinn <-  tweets_tidy_df %>% 
          filter(!str_detect(text, "^RT")) %>%
          mutate(text = str_replace_all(text, replace_reg, ""),
          line = row_number()) %>%
          unnest_tokens(word, text, token = "regex") %>%
          anti_join(stop_words) %>%
          filter(!word %in% mystopwords,
          str_detect(word, "[a-z]"))%>%
          inner_join(get_sentiments("afinn")) %>% 
          group_by(index = line %/% 20) %>% 
          summarise(sentiment = sum(score)) %>% 
          mutate(method = "AFINN")
################# Comparação de Dicionário     #################     
bing_and_nrc <- bind_rows(tweets_tidy_df %>%
                            filter(!str_detect(text, "^RT")) %>%
                            mutate(text = str_replace_all(text, replace_reg, ""),
                            line = row_number()) %>%
                            unnest_tokens(word, text, token = "regex") %>%
                            anti_join(stop_words) %>%
                            filter(!word %in% mystopwords,
                                     str_detect(word, "[a-z]"))%>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                            tweets_tidy_df %>%
                            filter(!str_detect(text, "^RT")) %>%
                            mutate(text = str_replace_all(text, replace_reg, ""),
                                     line = row_number()) %>%
                            unnest_tokens(word, text, token = "regex") %>%
                            anti_join(stop_words) %>%
                            filter(!word %in% mystopwords,
                                     str_detect(word, "[a-z]"))%>% 
                            inner_join(get_sentiments("nrc") %>% 
                            filter(sentiment %in% c("positive","negative"))) %>%
                            mutate(method = "NRC")) %>%
                            count(method, index = line %/% 20, sentiment) %>%
                            spread(sentiment, n, fill = 0) %>%
                            mutate(sentiment = positive - negative)

bind_rows(afinn,bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  ylab("Score")+
  xlab("Index")+
  facet_wrap(~method, ncol = 1, scales = "free_y") 
```
</div>

A comparação entre os três dicionários corrobora o elevado caráter negativo das palavras utilizadas. É relevante ressaltar que, embora o dicionário **afinn** apresente os menores scores, é o dicionário que aponta mais scores positivos para grupos de 20 palavras. No caso aqui considerado os dicionários evidenciam resultados parecidos entre si. Para fins de comparação utilizou-se, para o vocabulário **nrc**, as palavras relacionadas aos sentimentos "positive" e "negative" do referido dicionário.

As análises realizadas até este ponto foram realizadas por meio de unigramas, isto é, análises de palavra por palavra em separado. Análises baseadas em unigramas podem não ser a melhor opção pois há composições de palavras que alteram completamente o sentido da frase. Um exemplo intuitivo e iminente são as palavras precedidas por "not". Quando este é o caso as análises de unigramas captam apenas o sentido de uma palavra, não há como saber se a referida palavra está precedida por "not", o que obviamente altera complemente o sentido da expressão. Para evitar tais problemas e obtermos as sequências de palavras mais frequentes no texto prosseguimos com uma análise baseada em bigramas, isto é, análises baseadas em composições de duas palavras ao invés de uma.

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
#### Bigramas

tidy_tweets_big <- tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "ngrams",n=2) %>%
  filter(!word %in% mystopwords,
         str_detect(word, "[a-z]"))

tidy_tweets_big %>%
  count(word, sort = TRUE)

bigrams_separated <- tidy_tweets_big %>%
  separate(word, c("word1", "word2"), sep = " ")

bigrams_separated %>%
  filter(word1 == "depression") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

# We can then examine the most frequent words that were preceded by “word”  

depression_words <- bigrams_separated %>%
  filter(word1 == "depression") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

depression_words

depression_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Palavras precedidas por \"depression\"") +
  ylab("Score * número de ocorrências") +
  coord_flip()
```
</div>

Em análise de bigramas algo relevante é a quantidade de vezes que alguma palavra de interesse, no caso "depression", é precedida de outras palavras. Observa-se a ocorrência de algumas palavras relacionadas positivas, sendo as principais "love" e "win". Pelo espectro negativo há de se ressaltar o elevado número de vezes em que a palavra "depression" é precedida de "anxiety", "kills" e palavras relacionadas a suicídio. Confirma-se, a priori, os achados nas análises de unigramas quando se comparou a ocorrência de palavras relacionadas a sentimentos do dicionário **nrc**. Naquele caso as palavras relacionadas a ansiedade e suicídio também apareceram com frequência elevada.

Ainda no contexto de bigramas há de se mencionar as redes de palavras. As redes de palavras são dispositivos visuais que auxiliam na compreensão da base de dados e dos sentimentos envolvidos.

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}

if("igraph" %in% rownames(installed.packages())==FALSE)
{install.packages("igraph")};library(igraph)
if("ggraph" %in% rownames(installed.packages())==FALSE)
{install.packages("ggraph")};library(ggraph)
### Rede de palavras

mystopwords_rede <- c(stop_words$word,"rn","bc","wcw","af","lol")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% mystopwords_rede) %>%
  filter(!word2 %in% mystopwords_rede)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph

set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

## REDE DE PALAVRAS
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n,edge_width = n), edge_colour = "darkred", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "royalblue", size = 5) +
  geom_node_text(aes(label = name), repel=TRUE, point.padding = unit(0.2, "lines")) +
  theme_void()
```
</div>

A rede de palavras apresenta três nós nos entornos das palavras "depression", "depressive" e "depressed". Algo desta natureza já era esperado uma vez que foram realizadas buscas que continham estas palavras. É interessante observar que palavras de cunho técnico, "clinical", "severe", "deep" e "anxiety" estão relacionadas com a palavra "depression", ao passo que palavras de cunho pejorativo se encontram mais próximas de "depressed" e "depressive" se conecta, na grande parte dos casos, com palavras relacionadas ao grau de depressão ou a eventos relacionados à doença. 

É relevante destacar que outras conexões de palavras aparecem com relativa frequência ao longo do texto, sendo as principais: "hard"->"time", "mental"->"health", "social"->"media" e "love"->"island". Com excessão da última, as demais conexões estão relacionadas, segundo a literatura pertinente, com depressão sendo "social media" uma aparição bastante interessante.

No contexto da análise de sentimentos já está evidente que ansiedade possui relação próxima com depressão e sentimentos relacionados. Abaixo realizamos uma análise de correlação entre as palavras cujo objetivo é identifica palavras mais correlacionadas com algum termo específico, no caso "anxiety". 

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
if("devtools" %in% rownames(installed.packages())==FALSE)
{install.packages("devtools")};library(devtools)
if("widyr" %in% rownames(installed.packages())==FALSE)
{install_github("dgrtwo/widyr")};library(widyr)

#Counting and correlating pairs of words with the widyr package
mystopwords_w_dep <- c(stop_words$word,"depression,")

word_cors <- tidy_tweets %>%
  group_by(word) %>%
  filter(!word %in% mystopwords_w_dep,
         str_detect(word, "[a-z]"),
         n() >= 15) %>%
  pairwise_cor(word, screenName, sort = TRUE)

#Palavras correlacionadas com depression
word_cors %>%
  filter(item1 %in% c("anxiety")) %>%
  group_by(item1) %>%
  top_n(8) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  ylab("Correlação")+
  xlab(NULL)+
  coord_flip()
```
</div>

As palavras mais correlacionadas com "anxiety" são "depression", "disorder" e "social". Mais uma vez se comprova a estreita relação entre ansiedade e depressão. A aparição da palavra "social" neste contexto é bastante interessante pois, segundo a rede de palavras, "social" é frequentemente seguido de "media", então conclui-se que há correlação positiva entre "social media" e "anxiety". Sendo assim observa-se que há estreita relação entre "social media" e depressão, relacão intermediada por "anxiety". 

## Análise de tópicos

Nesta etapa será realizada uma análise de tópicos para verificar se o método LDA é capaz de encontrar algum agrupar os tweets de alguma forma. Na análise, cada usuário do Twitter será visto como o "documento" do qual o texto faz parte. Para isso, criamos um banco de dados onde as palavras e suas frequências são agrupadas por usuários, identificados pelo $\texttt{screenName}$. Adicionalmente, será feita a suposição de que os textos fazem parte de dois grupos desconhecidos e através da função $\texttt{LDA}$ do pacote $\texttt{topicmodels}$, serão obtidas as probabilidades de cada palavra e de cada usuário (documento) pertencer a cada um dos grupos, que serão identificados pelos números 1 e 2.

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
if("topicmodels" %in% rownames(installed.packages())==FALSE)
{install.packages("topicmodels")};library(topicmodels)

# Criando um tidy com contando as palavras por usuário
tidy_data <- tidy_tweets %>%
    group_by(screenName) %>%
    count(word, sort = TRUE) %>%
    filter(n >= 0)
# criando um DocumentTermMatrix para ser usado na função LDA
depre_dtm <- tidy_data %>% cast_dtm(screenName, word, n)

# aplicando a análise de tópicos
depre_lda <- LDA(depre_dtm, k = 2, control = list(seed = 1234))

# usando o tidy para extrair as probabilidades tópico/palavra
depre_topics <- tidy(depre_lda, matrix = "beta")
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
# aqui extraímos as palavras com maiores probabilidades de pertecerem
# a cada um dos tópicos
depre_top_terms <- depre_topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
# gerando o gráfico
depre_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  ylab(expression(beta))+
  xlab("Termo")+
  coord_flip()
```
</div>

Após o método LDA ser aplicado, o índice $\texttt{beta}$ fornece a probabilidade de cada palavra pertencer a um dos grupos. Durante a análise, notamos que não parece haver grupos distintos bem definidos nos tweets analisados, sendo os grupos 1 e 2 bastante similiras e com várias palavras em comum. Tal resultados também foi verificado quando se connsidera números maiores ($k$) de grupos desconhecidos. No gráfico acima são apresentadas para cada grupo identificado pelo LDA as palavras que possuem as maiores de $\texttt{beta}$. Apesar dos grupos 1 e 2 serem bastante parecidos, podemos observar que o primeiro grupo parece estar mais relacionado com o estado "depressed", enquanto no segundo grupo tem-se uma ênfase maior no sentimento "depression".

<div class="fold s">
```{r message=FALSE,echo=TRUE}
# Aqui calculamos a probabilidade de cada pessoa escrever sobre
# cada um dos tópicos
depre_topics_id <- tidy(depre_lda, matrix = "gamma")
depre_topics_id %>% group_by(topic) %>%
  top_n(3) %>% arrange(topic, desc(gamma))

# Aqui analisamos algumas das pessoas com as maiores probabilidades de 
# escrever sobre algum dos tópicos

# Exemplo 1
exemplo1 <- tweets.df %>% filter(screenName == "_Depressed_Zuka") %>%
  select(text)
exemplo1$text[15]

# Exemplo 2
exemplo2 <- tweets.df %>% filter(screenName == "Quixotitron_4") %>%
  select(text)
exemplo2$text[1]

# Exemplo 3
exemplo3 <- tweets.df %>% filter(screenName == "RF_NYC_2010") %>%
  select(text)
exemplo3$text[5]

# Exemplo 4
exemplo4 <- tweets.df %>% filter(screenName == "DepressionRoots") %>%
  select(text)
exemplo4$text[2]
```
</div>

Como mencionado, pelo LDA também podemos estimar uma probabilidade que cada usuário tem de pertencer a cada um dos grupos. De maneira similar como foi feito para as palavras, no código anterior identificamos dentro dos grupos 1 e 2 quais pessoas tinham as maiores probabilidades, que é dada pelo índice $\texttt{gamma}$. Acima são mostrados alguns textos dos usuários identificados pelo LDA, onde podemos ver que alguns dos textos parecem ser provenientes de pessoas com um nível expressivo de descontentamento ou tristeza, como os exemplos 1 e 3. No exemplo 2 temos um exemplo onde o usuário escreve diversos trechos relacionados ao livro Dom Quixote de Miguel de Cervantes, onde seus textos podem ter algum grau de tristeza mas não necessariamente indicar que a pessoa tenha depressão. No exemplo 4, o usuário escreve sobre depressão, o que pode explicar o fato dele ser incluído no grupo sobre o sentimento "depression". Em geral, notamos que o método LDA apresenta dificuldade em encontrar grupos dentro dos tweets analisados, mas que a medida de probabilidade para cada pessoa ainda pode fornecer um indicativo se cada pessoa está apresentando textos que se destacam dos demais, o que poderia ser um possível indício de sentimentos negativos que estajam relacionados ao estado de depressão.

## Dendogramas

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
if("qdap" %in% rownames(installed.packages())==FALSE)
{install.packages("qdap")};library(qdap)
if("dendextend" %in% rownames(installed.packages())==FALSE)
{install.packages("dendextend")};library(dendextend)

new_stops <-c(mystopwords,"lol","ur","bktwtr","booktweeter","im","https")
new_stops2 <-c(mystopwords,"depression", "lol","ur","bktwtr","booktweeter","amp","im","https","depressed","depression","depressive","depression")

# matriz de frequência de palavras
mat_freq  <- wfm(tidy_tweets$word)
mat_freq  <- as.matrix(mat_freq) #matriz de frequências
term_freq <-rowSums(mat_freq)
term_freq <-sort(term_freq,decreasing = T) # lista com as top palavras em orderm decrescente

bardepless  <- barplot(term_freq[1:50],col="tan",las=2) #vizualização das palavras mais frequentes
```
</div>

Nota-se que as expressões *depressed*, *depression* e *depressive* são as mais frequentes, aparecendo, 5007, 3936 e 1784 vezes respectivamente. Este resultado é o esperado, visto que foram justamente as três palavras utilizadas como filtro para a seleção dos tweets que compõe a base de dados do trabalho. A fim de se analisar de forma mais precisa a frequência deste subconjunto de dados, optou-se por retirar estas três observações a fim de tornar a escala de frequência da demais palavras mais clara, como pode-se observar abaixo.

Nota-se que aproximadamente as 20 primeiras palavras possuem um nivel de diferença maior com relação às trinta ultimas palavras. Para uma análise de dendrograma, esta diferença na frequência é de grande signigicância à medida em que os grupos são formados a partir das mesmas. Além disso, nota-se um grande número de palavras que expressam sentimentos, o que revela um padrão dos textos da base selecionada que se relaciona à saúde mental.

Isto posto o próximo passo, foi a construção do dendrograma, com as cinquenta palavras mais frequentes, como exposto abaixo.

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE,eval=FALSE}
dend1 <- hclust((dist(term_freq[1:50],method="euclidean"))) #apenas 50 palavras mais frequentes
plot(dend1,ylab = "Distância",las=0)
```
</div>

O método selecionado de cálculo das distâncias entre as observações, no caso a distância entre as frequências, foi o euclidiano. Nota-se que há dois grandes grupos de palavras, que se subdividem em pelo menos mais dois grandes grupos, formando os chamados *clusters*. Além disso, pode-se notar que, como esperado, há um grande número de observações na mesma altura do dendrograma, que representam as aproximadamente trinta palavras com frequência semelhantes. Uma das vantagens da construção do dendrograma é o fato de que não é necesessário se definir, a priori, o número de grupos que o conjunto de palavras possui. A partir da análise desde dendrograma, optou-se por definir de forma a priori, o número de clusters, de forma a tornar possível sua vizualização de forma mais clara. O segundo dendrograma abaixo, ilustra o caso com oito grupos:

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
par(bg="white")
dend4.2 <- dist(term_freq[1:50]) %>% 
  dist %>% 
  hclust %>%
  as.dendrogram %>% 
  set("branches_k_color",value=c("aquamarine2","chocolate1","darkcyan","brown2",
                      "orange1","purple1","palegreen3","royalblue1") ,k=8) %>%
  set("labels_colors",k=8,value=c("aquamarine2","chocolate1","darkcyan",
                                  "brown2","orange1","purple1","palegreen3",
                                   "royalblue1")) %>%
  set("labels_cex", c(.5,.5)) %>%  
  set("nodes_pch", 1,k=8)

plot(dend4.2)
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
dend2 <- dist(term_freq[1:20]) %>% dist %>% hclust %>% as.dendrogram %>% 
  set("branches_k_color",k=5) %>%
  plot(main = "Dendrograma com 5 clusters",las=1)
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
#DENDRO 3 - adicionando cores a 5 clusters HORIZONTAL (observação visual)
dend3 <- dist(term_freq[1:20]) %>% dist %>% hclust %>% as.dendrogram %>% 
  set("branches_k_color", k = 5)  %>% 
  set("labels_cex",.2) %>%
  hang.dendrogram(hang_height = -1)
plot(dend3, horiz = T)
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
#DENDRO 4 - cores nos labels
par(bg="white")
dend4 <- dist(term_freq[1:50]) %>% dist %>% hclust %>% as.dendrogram %>% 
  set("branches_k_color", k=4) %>%
  set("labels_colors",k=4) %>% set("labels_cex", c(.5,.5)) %>%  
  set("nodes_pch", 1)
plot(dend4)
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
par(bg="white")
dend4.2 <- dist(term_freq[1:50]) %>% dist %>% hclust %>% as.dendrogram %>% 
  set("branches_k_color", value=c("aquamarine2", "chocolate1","darkcyan","brown2","orange1","purple1","palegreen3","royalblue1") ,k=8) %>%
  set("labels_colors",k=8, value=c("aquamarine2", "chocolate1","darkcyan","brown2","orange1","purple1","palegreen3","royalblue1") ) %>% set("labels_cex", c(.5,.5)) %>%  
  set("nodes_pch", 1,k=8)
plot(dend4.2)
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
#dendro 5 - radial - dar cores
if("ape" %in% rownames(installed.packages())==FALSE)
{install.packages("ape")};library(ape)

dend5 <- dist(term_freq[1:50]) %>% dist %>% hclust %>% as.dendrogram
plot(as.phylo(dend5), type="fan")
```
</div>

<div class="fold s">
```{r message=FALSE,results='hide',echo=TRUE}
#if("corrplot" %in% rownames(installed.packages())==FALSE)
#{install.packages("corrplot")};library(corrplot)

#corrplot(cor.dendlist(dend1234),"pie","lower")
#par(mfrow = c(1,2),bg="white")
#plot(dend1, main = "Complete")
#plot(dend2, main = "Single" )
#par(mfrow = c(1,1))
```
</div>

*escolher qual dendograma usar*

[^1]: ver https://www.merriam-webster.com/dictionary/dendrogram
[^2]: As palavras extras exluídas das análise foram: "https", "im" "ur", "lol", "bktwtr", "booktweeter".

# Referências bibliográficas


