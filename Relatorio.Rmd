---
title: "Seminário - Mineração de Texto"
author: "Fabiana, Fernando, Mário, Rafael e Rodney"
date: "Junho de 2017"
output: 
  html_document: 
    highlight: haddock
    number_sections: yes
    theme: journal
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
      smooth_scroll: no
bibliography: bibliografia.bib
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Metodologia

## Descrição dos dados
A base de dados utilizada contém 30000 tweets extraídos com auxílio da função $\texttt{searchTwitter}$ do pacote $\texttt{twitteR}$. O banco de dados é composto por 10000 tweets escritos em inglês durante o período que compreende 01/01/2015 a 06/06/2017, contendo as palavras "depression", "depressive" e "depressed". A função $\texttt{searchTwitter}$ retorna uma lista com diversas informações, então faz-se necessário aplicar a função $\texttt{twListToPdf}$, também disponível no pacote $\texttt{twitteR}$, para transformar a lista do twitter em um dataframe compatível com os métodos  $\texttt{tidy}$ e $\texttt{tm}$.

## Análise de tópicos

Um interesse usual em análises de grandes volumes de textos é classicar os documentos em diferentes tópicos, permitindo idetenficar ou validar grupos entre os documentos analisados.  Uma forma de realizar esse tipo de análise é utilizar o método de alocação latente de Dirichlet (LDA, na sigla em inglês), que foi proposto por @blei2003latent. Esse método é bastante conhecido na literatura de mineração de texto pela sua eficiência, já tendo sido aplicado em análises de e-mails, livros digitais e notícias, por exemplo, como destacam @berry2010text.

As principais características do LDA, como apresentam @Silge2017Text, são as seguintes:

* Documentos diferentes são formados por misturas de tópicos: Dados dois documentos $A$ e $B$, e tópicos distintos 1 e 2, podemos ter que o $A$ é formado em 75% por conteúdo sobre o tópico 1 e em 25% sobre o tópico 2, enquanto $B$ é formado em 40% sobre o tópico 1 e 60% sobre o tópico 2, por exemplo.

* Diferentes palavras podem fazer parte de diferentes tópicos: a palavra "estudante" pode estar mais associada a um tópico sobre universidades enquanto que "cliente" pode ser mais associado a um tópico sobre empresas, mas o termo "dados" pode estar relacionado de maneira similar a ambos esses tópicos.

Para exemplificar a suposição do modelo LDA vamos considerar as seguintes notações: uma palavra $w$ é uma unidade discreta de um conjunto que forma o vocabulário utilizado; um documento $\mathbf{w} = \left(w_1,\ldots,w_N\right)$ é um conjunto de $N$ palavras; um corpo $D=\left\{\mathbf{w}_1,\ldots,\mathbf{w}_M\right\}$ é um conjunto de $M$ documentos. Adicionalmente, no LDA o número de tópicos existentes é assumido conhecido, mas os tópicos em si são variáveis latentes que não conhecemos e desejamos estimar. Dessa maneira, @blei2003latent consideram as seguintes suposições na geração do corpo:

1. Gera-se $N\sim \textrm{Pois}\left(\lambda\right)$;
2. Gera-se $\theta = \textrm{Dirichlet}\left(\mathbf{\alpha}\right)$, o qual será o vetor de probabilidades dos $k$ tópicos;
3. Para cada uma das $N$ palavras $w_n$:

+ Sorteia-se um tópico $z_n \sim \textrm{Multinomial}\left(k,\theta\right)$

+ Sorteia-se uma palavra $w_n$ de $p\left(w_n|z_n,\beta\right)$, que segue uma distribuição multinomial cujas probabilidades são condicionadas no tópico $z_n$ e em um vetor desconhecido de parâmetros $\beta$.


A aplicação do LDA apresentada por @blei2003latent é baseada em um modelo Bayesiano com três níveis hierárquicos, e permite estimar a probabilidade que cada palavra tem de pertencer à cada um dos tópicos, bem como estimar a probabilidade de cada documento ser sobre cada tópico.

No programa $\texttt{R}$ a análise de tópicos pode ser realizada através do pacote $\texttt{topicmodels}$, utilizando a função $\texttt{LDA}$. Uma aplicação dessa função do $\texttt{R}$ é fornecida por @Silge2017Text em um exemplo onde livros distintos são tratados como tópicos e seus respectivos capítulos como documentos; no exemplo dos autores, mostra-se que o LDA consegue classificar quase todos capítulos corretamente em seus respectivos livros, demonstrando a eficiência do método.

## Análise de sentimentos

Em análise de sentimentos o principal objetivo consiste em fazer "inferência"" sobre sentimetos expressos em forma de texto. @messias2017 cita algumas das recentes aplicações da referida metodologia, sendo as principais:

* Monitorar a reputação de alguma empresa ou marca;
* Fornecer perspectivas analíticas para investidores sobre determinado mercado (se certo mercado tem potencial ou não);
* Rastrear opiniões associadas a certos políticos (tais informações são úteis para o marketing do candidato) e
* Proporcionar meios de mensurar o bem-estar dos usuários de certo modelo de smartphone.

@goncalves2013 discute que não há a "melhor" técnica para se aplicar em análise de sentimentos. Conclusão semelhante foi obtida por @ribeiro2016 após comparar 22 métodos diferentes em 18 documentos onde não foi possível encontrar um método que fosse igualmente bom, em termos de classificação, para todos os textos. 

Na aplicação sobre depressão iremos considerar os dicionários de sentimentos **afinn**(@afinn2011), **bing**(@bing2005) e **nrc**(@nrc2013), por estarem disponíveis no R. Os três dicionários são compostos por unigramas, i.e., palavras únicas. Os vocabulários contém palavras em que são associados scores relativos a sentimentos positivos ou negativos. No caso do **nrc** há associação de palavras a sentimentos como alegria, raiva, tristeza, negatividade, surpresa, medo e desgosto. Esta classificação é feita de modo binário (sim ou não), se determinada palavra corresponde a determinado sentimento. Já no vocabulário **bing** as palavras são classificadas, também de modo binário, porém não há distinção de sentimentos. As palavras ou são associadas a sentimentos positivos ou a negativos. Por fim, o vocabulário **afinn** atribui scores que vão de -5 a 5 para as palavras. De modo que quanto mais próximo de -5 mais negativa é aquela palavra e, de modo análogo, quanto mais próximo de 5 mais posiva é a palavra. Os três vocabulários estão disponívels no $\texttt{R}$ no pacote $\texttt{tidytext}$. 

Geralmente os dicionários são elaborados pelos próprios autores com auxílio de crowdsourcing. É haver algum tipo de validação cruzada para testar o dicionário criado. Neste contexto fica evidente que os vocabulário são contemporâneos, ou seja, não é recomendado aplicar um dicionário de sentimentos atual para analisar um texto de séculos atrás. A análise pode ser feita, mas devem ser tomados os devidos cuidados ao analisar os achados.

Iniciamos a análise de sentimentos com o vocabulário **nrc** para obtermos palavras relacionadas a cada um dos sentimos, sendo eles alegria, raiva, tristeza, negatividade, surpresa, medo e desgosto, de modo que obtemos gráficos de contagem de palavras para cada um deles. Essa técnica permite avaliar quais palavras estão associadas a tais sentimentos no contexto de depressão. Prosseguimos a análise de sentimentos com o dicionário **bing**, sendo possível construir índices baseados nos tipos de palavras (positiva ou negativa) e na quantidade de ocorrência das mesmas em um grupo de $n$ linhas. Esse tipo de técnica permite avaliar alterações, caso existam, de sentimentos ao longo do texto. Note que a quantidade de linhas que serãoagrupadas depende do tipo e do tamanho do texto que está sendo análisado. Ainda utilizando o dicionário **bing**, apresentamos uma núvem de palavras de sentimentos, isto é, uma núvem de palavras com distinção entre palavras positivas e negativas. Prosseguimos a análise de sentimentos comparando os três dicionários, de modo a observar se há alguma discrepância em algum deles. 

# Resultados e discussões

## Análise de sentimentos

```{r, message=FALSE,results='hide',echo=TRUE}

if("dplyr" %in% rownames(installed.packages())==FALSE)
{install.packages("dplyr")};library(dplyr)

if("stringr" %in% rownames(installed.packages())==FALSE)
{install.packages("stringr")};library(stringr)

if("tidytext" %in% rownames(installed.packages())==FALSE)
{install.packages("tidytext")};library(tidytext)

if("tidyr" %in% rownames(installed.packages())==FALSE)
{install.packages("tidyr")};library(tidyr)

if("ggplot2" %in% rownames(installed.packages())==FALSE)
{install.packages("ggplot2")};library(ggplot2)

if("twitteR" %in% rownames(installed.packages())==FALSE)
{install.packages("twitteR")};library(twitteR)

if("tibble" %in% rownames(installed.packages())==FALSE)
{install.packages("tibble")};library(tibble)

udemytweets <-readRDS("depression_tweets_final.gzip")

tweets.df <- twListToDF(udemytweets) ##CALMA QUE VAI DEMORAR

tweets_tidy_df <- as_tibble(tweets.df)
# Limpando a base de dados

replace_reg <- "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT|https"
unnest_reg  <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"

data("stop_words")

mystopwords <- c(stop_words$word)

tidy_tweets <- tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "regex") %>%
  filter(!word %in% mystopwords,
         str_detect(word, "[a-z]")) %>%
  anti_join(stop_words)

nrneg <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

tidy_tweets %>%
  inner_join(nrneg) %>%
  count(word, sort = TRUE)%>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip()
```

Analisar o gráfico

```{r message=FALSE,results='hide',echo=TRUE}
nrneg <- get_sentiments("nrc") %>% 
  filter(sentiment == "negative")

tidy_tweets %>%
  inner_join(nrneg) %>%
  count(word, sort = TRUE)%>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip()
```

```{r message=FALSE,results='hide',echo=TRUE}

#############################################
#####         sadness                    ####   
nrsad <- get_sentiments("nrc") %>% 
  filter(sentiment == "sadness")

tidy_tweets %>%
  inner_join(nrsad) %>%
  count(word, sort = TRUE) %>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
    xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip()
```
  
```{r message=FALSE,results='hide',echo=TRUE}
#############################################
#####         anger                      ####   
nrang <- get_sentiments("nrc") %>% 
  filter(sentiment == "anger")


tidy_tweets %>%
  inner_join(nrang) %>%
  count(word, sort = TRUE) %>%
  filter(n > 15 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
    xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip() 
```

```{r message=FALSE,results='hide',echo=TRUE}

#############################################
#####         fear                       ####   
nrfear <- get_sentiments("nrc") %>% 
  filter(sentiment == "fear")

tidy_tweets %>%
  inner_join(nrfear) %>%
  count(word, sort = TRUE) %>%
  filter(n > 25 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
    xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip() 
```

```{r message=FALSE,results='hide',echo=TRUE}
#############################################
#####         disgust                    ####   
nrdisg <- get_sentiments("nrc") %>% 
  filter(sentiment == "disgust")

tidy_tweets %>%
  inner_join(nrdisg) %>%
  count(word, sort = TRUE) %>%
  filter(n > 15 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
    xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip() 
```

```{r message=FALSE,results='hide',echo=TRUE}
#############################################
#####         surprise                   ####   
nrsurp <- get_sentiments("nrc") %>% 
  filter(sentiment == "surprise")

tidy_tweets %>%
  inner_join(nrsurp) %>%
  count(word, sort = TRUE) %>%
  filter(n > 5 & n < 500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
    xlab(NULL) +
  ylab("Número de ocorrências")+
  coord_flip() 
```

```{r message=FALSE,results='hide',echo=TRUE}
#################################################################
################# Dicionário Bing               #################     
 tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, ""),
         line = row_number()) %>%
  unnest_tokens(word, text, token = "regex") %>%
  anti_join(stop_words) %>%
  filter(!word %in% mystopwords,
         str_detect(word, "[a-z]"))%>%
  inner_join(get_sentiments("bing"))%>%
  count(index = line %/% 150, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  ggplot(aes(index, sentiment)) +
  geom_col(show.legend = FALSE) +
  xlab("Index")+
  ylab("Sentimento")
```


```{r message=FALSE,results='hide',echo=TRUE}
#################################################################
################# Dicionário afinn              #################     

afinn <-  tweets_tidy_df %>% 
          filter(!str_detect(text, "^RT")) %>%
          mutate(text = str_replace_all(text, replace_reg, ""),
          line = row_number()) %>%
          unnest_tokens(word, text, token = "regex") %>%
          anti_join(stop_words) %>%
          filter(!word %in% mystopwords,
          str_detect(word, "[a-z]"))%>%
          inner_join(get_sentiments("afinn")) %>% 
          group_by(index = line %/% 20) %>% 
          summarise(sentiment = sum(score)) %>% 
          mutate(method = "AFINN")
#################################################################
################# Comparação de Dicionário     #################     
bing_and_nrc <- bind_rows(tweets_tidy_df %>%
                            filter(!str_detect(text, "^RT")) %>%
                            mutate(text = str_replace_all(text, replace_reg, ""),
                            line = row_number()) %>%
                            unnest_tokens(word, text, token = "regex") %>%
                            anti_join(stop_words) %>%
                            filter(!word %in% mystopwords,
                                     str_detect(word, "[a-z]"))%>% 
                            inner_join(get_sentiments("bing")) %>%
                            mutate(method = "Bing et al."),
                            tweets_tidy_df %>%
                            filter(!str_detect(text, "^RT")) %>%
                            mutate(text = str_replace_all(text, replace_reg, ""),
                                     line = row_number()) %>%
                            unnest_tokens(word, text, token = "regex") %>%
                            anti_join(stop_words) %>%
                            filter(!word %in% mystopwords,
                                     str_detect(word, "[a-z]"))%>% 
                            inner_join(get_sentiments("nrc") %>% 
                            filter(sentiment %in% c("positive","negative"))) %>%
                            mutate(method = "NRC")) %>%
                            count(method, index = line %/% 20, sentiment) %>%
                            spread(sentiment, n, fill = 0) %>%
                            mutate(sentiment = positive - negative)

bind_rows(afinn,bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  ylab("Score")+
  xlab("Index")+
  facet_wrap(~method, ncol = 1, scales = "free_y") 
```

```{r message=FALSE,results='hide',echo=TRUE}
#### Bigramas

tidy_tweets_big <- tweets_tidy_df %>% 
  filter(!str_detect(text, "^RT"),
         text != "im",
         text != "ur",
         text != "af",
         text != "wcw",
         text != "shes",
         text != "booktweeter0",
         text != "bktwr",
         !str_detect(text, "[0-9]"),
         !str_detect(text, ":"),
         !str_detect(text, "textless")) %>%
  mutate(text = str_replace_all(text, replace_reg, "")) %>%
  unnest_tokens(word, text, token = "ngrams",n=2) %>%
  filter(!word %in% mystopwords,
         str_detect(word, "[a-z]"))

tidy_tweets_big %>%
  count(word, sort = TRUE)

bigrams_separated <- tidy_tweets_big %>%
  separate(word, c("word1", "word2"), sep = " ")

bigrams_separated %>%
  filter(word1 == "depression") %>%
  count(word1, word2, sort = TRUE)

AFINN <- get_sentiments("afinn")

# We can then examine the most frequent words that were preceded by “not” and 
# were associated with a sentiment.
depression_words <- bigrams_separated %>%
  filter(word1 == "depression") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, score, sort = TRUE) %>%
  ungroup()

depression_words

depression_words %>%
  mutate(contribution = n * score) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * score, fill = n * score > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Palavras precedidas por \"depression\"") +
  ylab("Score * número de ocorrências") +
  coord_flip()
```


```{r message=FALSE,results='hide',echo=TRUE}

if("igraph" %in% rownames(installed.packages())==FALSE)
{install.packages("igraph")};library(igraph)
if("ggraph" %in% rownames(installed.packages())==FALSE)
{install.packages("ggraph")};library(ggraph)
### Rede de palavras

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph

set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

## REDE DE PALAVRAS
ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n,edge_width = n), edge_colour = "darkred", show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "royalblue", size = 5) +
  geom_node_text(aes(label = name), repel=TRUE, point.padding = unit(0.2, "lines")) +
  theme_void()
```

```{r message=FALSE,results='hide',echo=TRUE}

if("devtools" %in% rownames(installed.packages())==FALSE)
{install.packages("devtools")};library(devtools)
if("widyr" %in% rownames(installed.packages())==FALSE)
{install_github("dgrtwo/widyr")};library(widyr)

#Counting and correlating pairs of words with the widyr package
mystopwords_w_dep <- c(stop_words$word,"depression,")

word_cors <- tidy_tweets %>%
  group_by(word) %>%
  filter(!word %in% mystopwords_w_dep,
         str_detect(word, "[a-z]"),
         n() >= 15) %>%
  pairwise_cor(word, screenName, sort = TRUE)

#Palavras correlacionadas com depression
word_cors %>%
  filter(item1 %in% c("anxiety")) %>%
  group_by(item1) %>%
  top_n(8) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  ylab("Correlação")+
  xlab("Palavras")+
  coord_flip()
```

# Referências bibliográficas

